========================================================================================================================================
R Toolbox containing the packages to run the algorithms and to produce the results presented in submitted the paper:
Estimation and Feature Selection in Mixtures of Generalized Linear Experts Models.
Ref: arXiv:1907.06994, July, 2019 by Tuyen Huynh and Faicel Chamroukhi.
Please cite the paper and the toolbox when using the code!
========================================================================================================================================
We introduce 3 packages for the Regularized Mixture of Experts models using the Lasso penalty:
	+ Gaussian: RMoE package;
	+ Poisson: PoissonRMoE package;
	+ Logistic: LogisticRMoE package.
Please open the R project file and press "Ctrl + Shift + B" to install package.

Each package has one main function:
	+ RMoE: RMoE(X, Y, K, Lambda, Gamma, option);
	+ PoissonRMoE: PoissonRMoE(X, Y, K, Lambda, Gamma, option);
	+ LogisticRMoE: LogisitcRMoE(X, Y, K, R, Lambda, Gamma, option);

where

+ X: matrix of explanatory variables. Each feature should be standardized to have mean 0 and variance 1. One must add the column vector (1,1,...,1) for the intercept variable;

+ Y: vector of the response variable. For the Gaussian case Y should be standardized. For multi-logistic model Y is numbered from 1 to R (R is the number of labels of Y);

+ K: number of experts (K > 1);

+ R: number of labels of Y, used only for the logistic model.

+ Lambda: penalty value for the experts. In this work, lambda[k] = Lambda, for all k in {1,..,K} (Lambda >= 0);

+ Gamma: penalty value for the gating network. Here, gambda[k] = Lambda, for all k in {1,..K-1} (Gamma >= 0);

+ option: we use two methods to maximizing the M-step: proximal Newton and proximal Newton-type method. 
	- For proximal Newton: option = 0;
	- For proximal Newton-type: option = 1;

======================
       RESULTS 
======================
The results are stored in 5 different ".txt" files:

+ Para.txt: contains the parameters for the experts and gating network, where the last (K-1) vectors are vectors of the gating network, the remains are vectors of the experts;

+ LOG.txt: the penalized log-likelihood value;

+ BIC.txt: the value of BIC;

+ MAXP.txt: the mixing proportions for each observation;

+ Restore data.txt: contains the input data and the classification class (the last column) for each observation.

+ Sigma.txt: the value of sigma. (For Gaussian model only).

In addition, the figure in "Plots Tab" of RStudio is the array of the penalized log-likelihood value after each iteration.

============================
	TESTING DATA SETS
============================
Please find the testing data sets in the folder "Testing Data".
+ "Gaussian" includes 3 data sets:
	"Gaussian Data.txt": one typical simulated data (lambda = 5, 	gamma = 5, K = 2);
	"Housing Data.txt": the housing data set (lambda = 42, gamma 	= 10 for K = 2 and lambda = 20, gamma = 10 for K = 3);
	"RB Data.txt": the residential building data set (lambda = 	15, gamma = 15 for K = 3).
+ "Poisson" contains 2 data sets: 
	"Poisson Data.txt": one typical simulated data (lambda = 20, 	gamma = 10, K = 2);
	"Cleveland.txt": the Cleveland data set (lambda = 10, 	gamma 	= 4, K = 2).
+ "Logistic" contains 3 data sets:
	"Logistic Data.txt": one typical simulated data (lambda = 3, 	gamma = 3, K = 2, R = 2);
	"Ionosphere Data.txt": Ionosphere data (lambda = 3, gamma = 	3, K = 2, R = 2);
	"Musk-1 Data.txt": Musk-1 data (lambda = 5, gamma = 5, K = 	2, R = 2).

============================
		EXAMPLE
============================
We demonstrate one example on how to use these functions. Here, we give a simple R code for the Gaussian Regularized MoE.
--------------R code-------------- 
setwd("D:/R code/ProxL-MoEv.1.1") #set the directory
library("RMoE")
Data = read.table("Housing Data.txt")
Data = as.matrix(Data) #using matrix format
dms = dim(Data)[2]
X = Data[,-dms] #including the first column with 1..1
Y = Data[,dms]
K = 2 #number of experts
Lambda = 42
Gamma = 10
opt = 0 #opt = 0: proximal Newton; opt = 1: proximal Newton-type
RMoE(X, Y, K, Lambda, Gamma, opt)
----------------------------------

For comments and questions, please send email to:
bao-tuyen.huynh@unicaen.fr

THANK YOU!
	